{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d5146e-65ad-4f3b-95c4-c7b6b47d7046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: company_data/C38U.SI_stock_price_history_non_adj.csv\n",
      "Successfully loaded data.\n",
      "\n",
      "Performing initial data preparation (Date conversion, setting index, numeric coercion)...\n",
      "\n",
      "--- Initial Data Snapshot ---\n",
      "            Open  High   Low  Close     Adj      Volume\n",
      "Date                                                   \n",
      "2019-12-31  2.43  2.47  2.43   2.46  1.8324   4710400.0\n",
      "2020-01-02  2.44  2.47  2.43   2.46  1.8324   8248900.0\n",
      "2020-01-03  2.46  2.47  2.43   2.45  1.8249   5668600.0\n",
      "2020-01-06  2.44  2.45  2.42   2.43  1.8100  10843500.0\n",
      "2020-01-07  2.44  2.47  2.43   2.46  1.8324  12045600.0\n",
      "\n",
      "--- Initial Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1271 entries, 2019-12-31 to 2024-12-31\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    1257 non-null   float64\n",
      " 1   High    1257 non-null   float64\n",
      " 2   Low     1257 non-null   float64\n",
      " 3   Close   1257 non-null   float64\n",
      " 4   Adj     1257 non-null   float64\n",
      " 5   Volume  1251 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 69.5+ KB\n",
      "None\n",
      "Total rows loaded: 1271\n",
      "\n",
      "--- Starting Data Cleaning Process ---\n",
      "\n",
      "Checking for non-trading rows (Volume 0/NaN or Close NaN)...\n",
      "Found 20 rows likely representing non-trading events.\n",
      "Removed 20 non-trading event rows.\n",
      "\n",
      "Checking for remaining duplicate dates after non-trading event removal...\n",
      "Found 0 rows with duplicate dates (counting all instances) for removal.\n",
      "No remaining duplicate dates found or removed.\n",
      "\n",
      "Total rows after all initial cleaning steps: 1251\n",
      "\n",
      "--- Data after Cleaning Snapshot ---\n",
      "            Open  High   Low  Close     Adj      Volume\n",
      "Date                                                   \n",
      "2019-12-31  2.43  2.47  2.43   2.46  1.8324   4710400.0\n",
      "2020-01-02  2.44  2.47  2.43   2.46  1.8324   8248900.0\n",
      "2020-01-03  2.46  2.47  2.43   2.45  1.8249   5668600.0\n",
      "2020-01-06  2.44  2.45  2.42   2.43  1.8100  10843500.0\n",
      "2020-01-07  2.44  2.47  2.43   2.46  1.8324  12045600.0\n",
      "\n",
      "--- Data Info after Cleaning ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1251 entries, 2019-12-31 to 2024-12-31\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    1251 non-null   float64\n",
      " 1   High    1251 non-null   float64\n",
      " 2   Low     1251 non-null   float64\n",
      " 3   Close   1251 non-null   float64\n",
      " 4   Adj     1251 non-null   float64\n",
      " 5   Volume  1251 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 68.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path for your non-adjusted stock price history\n",
    "input_file_path = 'company_data/C38U.SI_stock_price_history_non_adj.csv'\n",
    "\n",
    "# --- 1. Load the Data from your CSV file ---\n",
    "print(f\"Attempting to load data from: {input_file_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    print(\"Successfully loaded data.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {input_file_path} was not found. Please check the path and filename.\")\n",
    "    exit() # Exit the script if the file isn't found\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Initial Data Preparation: Ensure Correct Data Types and Set Index ---\n",
    "print(\"\\nPerforming initial data preparation (Date conversion, setting index, numeric coercion)...\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed', dayfirst=True) \n",
    "# df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True).dt.date\n",
    "\n",
    "# Set 'Date' as the index and sort it, which is crucial for time-series operations\n",
    "df = df.set_index('Date').sort_index()\n",
    "\n",
    "# Ensure Open, High, Low, Close, and Volume columns are numeric.\n",
    "# Using errors='coerce' will turn any non-convertible values into NaN (Not a Number).\n",
    "df['Open'] = pd.to_numeric(df['Open'], errors='coerce')\n",
    "df['High'] = pd.to_numeric(df['High'], errors='coerce')\n",
    "df['Low'] = pd.to_numeric(df['Low'], errors='coerce')\n",
    "df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "\n",
    "# Remove commas from the 'Volume' column first, then convert to numeric\n",
    "df['Volume'] = df['Volume'].astype(str).str.replace(',', '', regex=False)\n",
    "df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n",
    "\n",
    "print(\"\\n--- Initial Data Snapshot ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Initial Data Info ---\")\n",
    "print(df.info())\n",
    "print(f\"Total rows loaded: {len(df)}\")\n",
    "\n",
    "\n",
    "# --- 3. Data Cleaning: Remove Non-Trading (e.g., Dividend/Split) Rows and Handle Duplicates ---\n",
    "\n",
    "print(\"\\n--- Starting Data Cleaning Process ---\")\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Step 3.1: Identify and Remove Rows Likely Representing Non-Trading Events\n",
    "# Common indicators for non-trading/dividend rows: Volume is 0 or NaN, or Close price is NaN.\n",
    "print(f\"\\nChecking for non-trading rows (Volume 0/NaN or Close NaN)...\")\n",
    "non_trading_rows_mask = (df['Volume'] == 0) | (df['Volume'].isna()) | (df['Close'].isna())\n",
    "\n",
    "num_non_trading_rows = non_trading_rows_mask.sum()\n",
    "print(f\"Found {num_non_trading_rows} rows likely representing non-trading events.\")\n",
    "\n",
    "# Keep only the rows that are NOT identified as non-trading events.\n",
    "df = df[~non_trading_rows_mask].copy() # Use .copy() to ensure it's a new DataFrame and avoid warnings\n",
    "\n",
    "rows_removed_events = initial_row_count - len(df)\n",
    "if rows_removed_events > 0:\n",
    "    print(f\"Removed {rows_removed_events} non-trading event rows.\")\n",
    "else:\n",
    "    print(\"No non-trading event rows found or removed.\")\n",
    "\n",
    "# Step 3.2: Handle Any Remaining Date Duplicates (after removing event rows)\n",
    "print(f\"\\nChecking for remaining duplicate dates after non-trading event removal...\")\n",
    "rows_before_duplicate_check = len(df)\n",
    "\n",
    "# Identify all rows where the index (Date) is duplicated, marking all occurrences.\n",
    "duplicate_dates_mask = df.index.duplicated(keep=False)\n",
    "num_duplicate_dates = duplicate_dates_mask.sum()\n",
    "print(f\"Found {num_duplicate_dates} rows with duplicate dates (counting all instances) for removal.\")\n",
    "\n",
    "# Filter the DataFrame to keep only non-duplicate index entries, keeping the first occurrence.\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "rows_removed_duplicates = rows_before_duplicate_check - len(df)\n",
    "if rows_removed_duplicates > 0:\n",
    "    print(f\"Removed {rows_removed_duplicates} remaining duplicate date entries.\")\n",
    "else:\n",
    "    print(\"No remaining duplicate dates found or removed.\")\n",
    "\n",
    "# Final check for any critical NaNs that might have been missed or introduced.\n",
    "# This ensures that essential columns for calculations are clean.\n",
    "df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "print(f\"\\nTotal rows after all initial cleaning steps: {len(df)}\")\n",
    "\n",
    "print(\"\\n--- Data after Cleaning Snapshot ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Data Info after Cleaning ---\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af5151c-815f-45b7-b20e-b53c640216ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# --- 1. Load and Prepare Data ---\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     df = pd.read_csv('calculated_averages/C38U.SI_stock_price_history_with_indicators.csv', parse_dates=['Date'], index_col='Date')\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# except FileNotFoundError:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     print(\"Error: CSV file not found. Please check that the file path is correct.\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#     exit()\u001b[39;00m\n\u001b[0;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import load_model # \n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "# try:\n",
    "#     df = pd.read_csv('calculated_averages/C38U.SI_stock_price_history_with_indicators.csv', parse_dates=['Date'], index_col='Date')\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Error: CSV file not found. Please check that the file path is correct.\")\n",
    "#     exit()\n",
    "\n",
    "data = df.filter(['Close'])\n",
    "dataset = data.values\n",
    "\n",
    "# Create and fit the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# --- 2. Load the Pre-trained Model ---\n",
    "# This replaces the 'Build Model' and 'Train Model' sections\n",
    "try:\n",
    "    print(\"\\n--- Loading the trained model from 'univariate_lstm_model_base_model.keras' ---\")\n",
    "    model = load_model('univariate_lstm_model_base_model.keras')\n",
    "    model.summary()\n",
    "except OSError:\n",
    "    print(\"Error: Model file 'univariate_lstm_model_base_model.keras' not found. Please run the training script first.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- THE REST OF THE SCRIPT FOR EVALUATION AND FORECASTING IS THE SAME ---\n",
    "\n",
    "# --- Define constants needed for the script ---\n",
    "SEQ_LEN = 180\n",
    "train_data_len = int(np.ceil(len(dataset) * .8))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# --- PART A: ONE-STEP-AHEAD EVALUATION ON TEST SET ---\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 3. Create the Testing Data Set ---\n",
    "test_data = scaled_data[train_data_len - SEQ_LEN:, :]\n",
    "x_test = []\n",
    "y_test = dataset[train_data_len:, :]\n",
    "\n",
    "for i in range(SEQ_LEN, len(test_data)):\n",
    "    x_test.append(test_data[i-SEQ_LEN:i, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "# --- 4. Get the Model's Predicted Price Values ---\n",
    "print(\"\\nEvaluating the loaded model on the test set...\")\n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# --- 5. Evaluate Model Performance ---\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "\n",
    "# --- 6. Visualize the Test Set Predictions ---\n",
    "train_plot_data = data[:train_data_len]\n",
    "valid_plot_data = data[train_data_len:].copy()\n",
    "valid_plot_data['Predictions'] = predictions\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Plot 1: (Loaded Model) One-Step-Ahead Prediction vs Actual Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price (SGD)')\n",
    "plt.plot(train_plot_data['Close'], label='Training History')\n",
    "plt.plot(valid_plot_data['Close'], label='Actual Price (Test Set)')\n",
    "plt.plot(valid_plot_data['Predictions'], label='Predicted Price')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# --- PART B: 90-DAY FORECAST BACKTEST ---\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 7. Perform Iterative Forecasting for Backtest ---\n",
    "N_STEPS_BACKTEST = 90\n",
    "print(f\"\\n--- Starting {N_STEPS_BACKTEST}-day forecast backtest with loaded model ---\")\n",
    "\n",
    "initial_sequence_backtest = scaled_data[train_data_len - SEQ_LEN : train_data_len, :]\n",
    "current_input_backtest = initial_sequence_backtest.reshape(1, SEQ_LEN, 1)\n",
    "backtest_predictions_scaled = []\n",
    "\n",
    "for i in range(N_STEPS_BACKTEST):\n",
    "    next_prediction_scaled = model.predict(current_input_backtest, verbose=0)[0, 0]\n",
    "    backtest_predictions_scaled.append(next_prediction_scaled)\n",
    "    \n",
    "    new_input_list = current_input_backtest[0, :, 0].tolist()\n",
    "    new_input_list.pop(0)\n",
    "    new_input_list.append(next_prediction_scaled)\n",
    "    current_input_backtest = np.array(new_input_list).reshape(1, SEQ_LEN, 1)\n",
    "\n",
    "# --- 8. Visualize the Backtest Forecast ---\n",
    "backtest_array = np.array(backtest_predictions_scaled).reshape(-1, 1)\n",
    "backtest_predictions_price = scaler.inverse_transform(backtest_array)\n",
    "\n",
    "forecast_start_date = data.index[train_data_len]\n",
    "backtest_dates = pd.date_range(start=forecast_start_date, periods=N_STEPS_BACKTEST)\n",
    "backtest_df = pd.DataFrame(backtest_predictions_price, index=backtest_dates, columns=['Backtest_Forecast'])\n",
    "\n",
    "train = data[:train_data_len]\n",
    "valid = data[train_data_len:]\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.title(f'Plot 2: (Loaded Model) LSTM Forecast Backtest vs. Actual Price ({N_STEPS_BACKTEST}-Day)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price (SGD)')\n",
    "plt.plot(train['Close'], label='Training History')\n",
    "plt.plot(valid['Close'], label='Actual Test Prices', color='orange')\n",
    "plt.plot(backtest_df['Backtest_Forecast'], label=f'{N_STEPS_BACKTEST}-Day Predicted Trend', color='red', linestyle='--')\n",
    "plt.axvline(x=forecast_start_date, color='grey', linestyle=':', linewidth=2, label='Forecast Start')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f33492-60d5-462d-a204-b491eb9a6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de203b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
